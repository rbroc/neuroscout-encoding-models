{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized (banded) CV regression workflow for Neuroscout\n",
    "This notebook implements an encoding model for a single subject using Regularized Ridge Regression, as implemented in https://github.com/gallantlab/himalaya. In Neuroscout, this same pipeline should be run for all subjects.\n",
    "- Input needed from the user\n",
    "    - Define datasets (independent model fitting for all datasets)\n",
    "    - Define cross-validation strategy\n",
    "        - Across runs\n",
    "        - Within runs\n",
    "    - Define estimator\n",
    "    - Define preprocessing steps (e.g., scaling?)\n",
    "    - Define bands\n",
    "    - Pass parameters\n",
    "    - Output: scores, parameters, predicted time series\n",
    "- Define outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyns\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = pyns.Neuroscout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Budapest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can explore the runs available in this dataset. Let's choose the first subject we see, and analyze all of their runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acquisition': None,\n",
       " 'dataset_id': 27,\n",
       " 'duration': 535.0,\n",
       " 'id': 1435,\n",
       " 'number': 3,\n",
       " 'session': None,\n",
       " 'subject': 'sid000005',\n",
       " 'task': 48,\n",
       " 'task_name': 'movie'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select subject from first run available in dataset\n",
    "api.runs.get(dataset_name='Budapest')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sid000005'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = api.runs.get(dataset_name='Budapest')[0]['subject']\n",
    "subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch predictors from Neuroscout and create design matrix\n",
    "Let's retrieve predictor events for multiple sets of predictors. \\\n",
    "For now, let's pick two sets: <b>MFCC</b> + <b>mel</b> features (plus some confounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = [f'mfcc_{i}' for i in range(20)]\n",
    "mel = [f'mel_{i}' for i in range(64)]\n",
    "confounds = ['rot_x', 'rot_y', 'rot_z', 'trans_x', 'trans_y', 'trans_z',\n",
    "             'a_comp_cor_00', 'a_comp_cor_01', 'a_comp_cor_02',\n",
    "             'a_comp_cor_03','a_comp_cor_04','a_comp_cor_05']\n",
    "\n",
    "all_vars = mfccs + mel + confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyns.fetch_utils import fetch_neuroscout_predictors, fetch_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fetch_neuroscout_predictors` will retrive the named predictors from the Neurscout API, and (optionally) resampled them to `TR`. All timepoints are concatenated into a single file, with identifying columns (i.e. `subject`, `run`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/src/pyns/pyns/endpoints/base.py:135: UserWarning: No API endpoint for stimulus_id, could not convert\n",
      "  warnings.warn(f\"No API endpoint for {col}, could not convert\")\n",
      "/home/alejandro/src/pyns/pyns/fetch_utils.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['amplitude'] = pd.to_numeric(df['amplitude'])\n"
     ]
    }
   ],
   "source": [
    "X_vars = fetch_neuroscout_predictors(\n",
    "    predictor_names=all_vars, dataset_name=dataset_name, subject=subject, \n",
    "    resample=True, return_type='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into design variables and meta-data\n",
    "X = X_vars[all_vars]\n",
    "X_meta = X_vars[X_vars.columns.difference(all_vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch fMRI data and load images\n",
    "\n",
    "To retrieve Neuroscout data, we use `datalad` to fetch the preprocessed images remote.\n",
    "pyNS includes a helper function to facilitate installing and fetching the dataset using datalad:f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/bids/layout/validation.py:51: UserWarning: The ability to pass arguments to BIDSLayout that control indexing is likely to be removed in future; possibly as early as PyBIDS 0.14. This includes the `config_filename`, `ignore`, `force_index`, and `index_metadata` arguments. The recommended usage pattern is to initialize a new BIDSLayoutIndexer with these arguments, and pass it to the BIDSLayout via the `indexer` argument.\n",
      "  warnings.warn(\"The ability to pass arguments to BIDSLayout that control \"\n",
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/bids/layout/validation.py:156: UserWarning: The PipelineDescription field was superseded by GeneratedBy in BIDS 1.4.0. You can use ``pybids upgrade`` to update your derivative dataset.\n",
      "  warnings.warn(\"The PipelineDescription field was superseded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action summary:\n",
      "  get (notneeded: 5)\n"
     ]
    }
   ],
   "source": [
    "preproc_dir, img_paths = fetch_images('Budapest', '/tmp/', subject=subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use pybids to identify the file we need to fetch, and use DataLad to fetch them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to prepare the images for analysis, we'll load them into a single array, and accompanying dataframe with meta-data for every volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stack_images(image_objects):\n",
    "    \"\"\" Stack images into single array, and collect metadata entities into dataframe \"\"\"\n",
    "    arrays = []\n",
    "    entities = []\n",
    "    image_objects = sorted(image_objects, key=lambda x: x.entities['run'])\n",
    "    for img in image_objects:\n",
    "        data = np.asanyarray(nib.load(img.path).dataobj)\n",
    "        run_y = data.reshape([data.shape[0] * data.shape[1] * data.shape[2], data.shape[3]]).T\n",
    "        arrays.append(run_y)\n",
    "        entities += [dict(img.entities)] * run_y.shape[0]\n",
    "    entities = pd.DataFrame(entities)\n",
    "    return np.vstack(arrays), entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, img_idx = _stack_images(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image volume\n",
    "Y[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datatype</th>\n",
       "      <th>desc</th>\n",
       "      <th>extension</th>\n",
       "      <th>run</th>\n",
       "      <th>space</th>\n",
       "      <th>subject</th>\n",
       "      <th>suffix</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>func</td>\n",
       "      <td>preproc</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>MNI152NLin2009cAsym</td>\n",
       "      <td>sid000005</td>\n",
       "      <td>bold</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>func</td>\n",
       "      <td>preproc</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>MNI152NLin2009cAsym</td>\n",
       "      <td>sid000005</td>\n",
       "      <td>bold</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>func</td>\n",
       "      <td>preproc</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>MNI152NLin2009cAsym</td>\n",
       "      <td>sid000005</td>\n",
       "      <td>bold</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>func</td>\n",
       "      <td>preproc</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>MNI152NLin2009cAsym</td>\n",
       "      <td>sid000005</td>\n",
       "      <td>bold</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>func</td>\n",
       "      <td>preproc</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>MNI152NLin2009cAsym</td>\n",
       "      <td>sid000005</td>\n",
       "      <td>bold</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datatype     desc extension  run                space    subject suffix  \\\n",
       "0     func  preproc   .nii.gz    1  MNI152NLin2009cAsym  sid000005   bold   \n",
       "1     func  preproc   .nii.gz    1  MNI152NLin2009cAsym  sid000005   bold   \n",
       "2     func  preproc   .nii.gz    1  MNI152NLin2009cAsym  sid000005   bold   \n",
       "3     func  preproc   .nii.gz    1  MNI152NLin2009cAsym  sid000005   bold   \n",
       "4     func  preproc   .nii.gz    1  MNI152NLin2009cAsym  sid000005   bold   \n",
       "\n",
       "    task  \n",
       "0  movie  \n",
       "1  movie  \n",
       "2  movie  \n",
       "3  movie  \n",
       "4  movie  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meta-data\n",
    "img_idx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold, PredefinedSplit\n",
    "from himalaya.ridge import GroupRidgeCV\n",
    "from himalaya.scoring import correlation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validated model fitting, prediction, and scoring loosely based on scikit-learn's [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). Returns a `results` dictionary with `'coefficients'`, `'test_predictions'`, and `'test_scores'` keys containing lists of numpy arrays for each outer cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_cv(estimator, X_vars, y, bands=None, groups=None,\n",
    "              scoring=correlation_score, cv=None,\n",
    "              inner_cv=None, confounds=None, split=None):\n",
    "    # Container for results\n",
    "    results = {\n",
    "        'coefficients': [],\n",
    "        'test_predictions': [],\n",
    "        'test_scores': []}\n",
    "    \n",
    "    # If confounds, stack at the end\n",
    "    if confounds is not None:\n",
    "        bands.append(confounds)\n",
    "    \n",
    "    if bands is not None:\n",
    "        X = []\n",
    "        for band in bands:\n",
    "            X.append(X_vars[band].as_matrix())\n",
    "    else:\n",
    "        X = X_vars.as_matrix()\n",
    "\n",
    "    # Extract number of samples for convenience\n",
    "    n_samples = y.shape[0]\n",
    "    \n",
    "    # Set default cross-validation to KFold if not specified\n",
    "    cv = KFold() if not cv else cv\n",
    "    \n",
    "    # Loop through outer cross-validation folds\n",
    "    for train, test in cv.split(np.arange(n_samples), groups=groups):\n",
    "        \n",
    "        # Get training model for list of model bands\n",
    "        X_train = [x[train] for x in X] if type(X) == list else X[train]\n",
    "        X_test = [x[test] for x in X] if type(X) == list else X[test]\n",
    "        \n",
    "        # Create inner cross-validation loop if specified\n",
    "        if inner_cv:\n",
    "            # Split inner cross-validation with groups if supplied\n",
    "            inner_groups = np.array(groups)[train] if groups else groups\n",
    "            inner_splits = inner_cv.split(np.arange(n_samples)[train],\n",
    "                                          groups=inner_groups)\n",
    "            \n",
    "            # Update estimator with inner cross-validator\n",
    "            estimator.set_params(cv=inner_splits)\n",
    "            print(np.unique(inner_groups))\n",
    "        \n",
    "        # Fit the regression model on training data\n",
    "        estimator.fit(X_train, y[train])\n",
    "        \n",
    "        # Zero out coefficients for confounds if provided\n",
    "        if confounds is not None:\n",
    "            estimator.coef_[-len(confounds):] = 0\n",
    "        \n",
    "        # Compute predictions with optional splitting by band\n",
    "        kwargs = {}\n",
    "        if split is not None:\n",
    "            kwargs['split'] = split\n",
    "        test_prediction = estimator.predict(X_test, **kwargs)\n",
    "        \n",
    "        # Test scores should also optionally split by band\n",
    "        test_score = scoring(y[test], test_prediction)\n",
    "        \n",
    "        # Populate results dictionary\n",
    "        results['coefficients'].append(estimator.coef_)\n",
    "        results['test_predictions'].append(test_prediction)\n",
    "        results['test_scores'].append(test_score)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Y[:, :100]\n",
    "\n",
    "# Default estimator should be GroupRidgeCV\n",
    "estimator = GroupRidgeCV(groups='input')\n",
    "\n",
    "bands = [mfccs, mel]\n",
    "confunds = confounds\n",
    "\n",
    "# Default cross-validation should be leave-one-run-out\n",
    "# Use `run_id` to define cross-validation strategy\n",
    "n_runs = len(X_meta['run_id'].unique())\n",
    "cv = GroupKFold(n_splits=n_runs)\n",
    "inner_cv = GroupKFold(n_splits=n_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3052, 100)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[........................................] 100% | 11.79 sec | 100 random sampling with cv | \n",
      "[1433 1434 1435 1437]\n",
      "[........................................] 100% | 13.84 sec | 100 random sampling with cv | \n",
      "[1434 1435 1436 1437]\n",
      "[........................................] 100% | 14.90 sec | 100 random sampling with cv | \n",
      "[1433 1434 1436 1437]\n",
      "[........................................] 100% | 14.70 sec | 100 random sampling with cv | \n",
      "[1433 1435 1436 1437]\n",
      "[........................................] 100% | 15.13 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "# Run model with specified cross-validation, groups, confounds, and split outputs\n",
    "results = _model_cv(estimator, X, y, cv=cv, bands=bands, inner_cv=inner_cv, groups=X_meta['run_id'].tolist(), split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 100)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['coefficients'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00145748,\n",
       "        -0.01041323, -0.02745176, -0.05784031, -0.00137035, -0.06062118,\n",
       "        -0.03022619, -0.07361765, -0.04044312,  0.02127786, -0.00946764,\n",
       "         0.01171312, -0.02639117, -0.0458949 ,  0.00366606, -0.00655776,\n",
       "        -0.02817272, -0.02004944,  0.04037386,  0.01514264,  0.03978828,\n",
       "         0.12336703,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.24099885,  0.00228771, -0.02030088],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.04790277,\n",
       "         0.03764841, -0.03439782,  0.01361495, -0.01362102, -0.00585526,\n",
       "        -0.01945454,  0.02494057,  0.00127206, -0.03788311, -0.00735007,\n",
       "        -0.01153186,  0.00104539,  0.05669522, -0.03441861, -0.04733022,\n",
       "         0.07812791,  0.00795943,  0.04034922, -0.09581939, -0.01918222,\n",
       "        -0.05849778,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.03779618,  0.03327169, -0.02854261]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test_scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[1433 1434 1435 1437]\n",
      "[1434 1435 1436 1437]\n",
      "[1433 1434 1436 1437]\n",
      "[1433 1435 1436 1437]\n"
     ]
    }
   ],
   "source": [
    "# Using single-band (non-banded) model with sklearn RidgeCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "results = _model_cv(RidgeCV(), X_vars, y, cv=cv, inner_cv=inner_cv, groups=X_meta['run_id'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate against other workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
